{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares three different ways of obtaining document embeddings:  \n",
    "\n",
    "- OpenAI (using text-embedding-3-large)\n",
    "- fasttext (using a 300-dimension embedding learned from the full corpus of papers)\n",
    "- sentence_transformers (using all-mpnet-base-v2)\n",
    "\n",
    "We select a random subset of 250 papers that are short enough to fit in the OpenAI context window (7500 tokens).\n",
    "\n",
    "We compare the results using representational similarity analysis, in which we first compute the correlation matrix across papers over each embedding, and the compute the correlation between the upper triangle elements in the correlation matrix.  This tells us how well the similarities between papers are matched across embeddings.\n",
    "\n",
    "TL/DR: the fasttext embedding was very close to the OpenAI embedding (r = 0.964).  Given that it's free we will use that for subsequent analyses.\n",
    "\n",
    "The fasttext embedding is saved at `<datadir>/fulltext.bin` for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ontology_learner.lit_mining.litmining_utils import get_fulltext_from_pmcid_json\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 132257 data files\n"
     ]
    }
   ],
   "source": [
    "datadir = Path('/Users/poldrack/Dropbox/data/ontology-learner/data/json')\n",
    "datafiles = list(datadir.glob('*.json'))\n",
    "print(f'found {len(datafiles)} data files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to generate a single "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fulltext = {}\n",
    "\n",
    "def process_fulltext(fulltext):\n",
    "    # remove all newlines\n",
    "    fulltext = fulltext.replace('\\n', ' ')\n",
    "    # remove all punctuation\n",
    "    fulltext = fulltext.translate(str.maketrans('', '', string.punctuation))\n",
    "    # remove all numbers\n",
    "    fulltext = re.sub(r'\\d+', '', fulltext)\n",
    "    # remove stopwords\n",
    "    fulltext = ' '.join([word for word in fulltext.split() if word not in stopwords.words('english')])\n",
    "    return fulltext\n",
    "\n",
    "outfile = Path('/Users/poldrack/Dropbox/data/ontology-learner/data/fulltext.pkl')\n",
    "if not outfile.exists():\n",
    "    for datafile in tqdm(datafiles):\n",
    "        pmcid = datafile.stem\n",
    "        fulltext[pmcid] = get_fulltext_from_pmcid_json(pmcid, datadir)\n",
    "        # processed_text = process_fulltext(fulltext[pmcid])\n",
    "        \n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(fulltext, f)\n",
    "else:\n",
    "    with open(outfile, 'rb') as f:\n",
    "        fulltext = pickle.load(f)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 132257 fulltext\n"
     ]
    }
   ],
   "source": [
    "print(f'found {len(fulltext)} fulltext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use openai embedding model to get embeddings for each paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenlength_file = Path('/Users/poldrack/Dropbox/data/ontology-learner/data/tokenlengths.pkl')\n",
    "if not tokenlength_file.exists():\n",
    "    import tiktoken\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "    tokenlengths = {}\n",
    "    for pmcid in tqdm(fulltext):\n",
    "        foo = enc.encode(fulltext[pmcid])\n",
    "        tokenlengths[pmcid] = len(foo)\n",
    "\n",
    "    with open(tokenlength_file, 'wb') as f:\n",
    "        pickle.dump(tokenlengths, f)\n",
    "else:\n",
    "    with open(tokenlength_file, 'rb') as f:\n",
    "        tokenlengths = pickle.load(f)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52309"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# this was .395 with previous version\n",
    "np.sum(np.array(list(tokenlengths.values())) < 7500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM embeddings\n",
    "\n",
    "As a first pass, we will get a few hundred document embeddings from openai, and compare them to the embeddings obtained using fasttext (learning directly on the full corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading openai embeddings from /Users/poldrack/Dropbox/data/ontology-learner/data/openai_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "openai_embeddings_file = Path('/Users/poldrack/Dropbox/data/ontology-learner/data/openai_embeddings.npy')\n",
    "sample_docs_file = Path('/Users/poldrack/Dropbox/data/ontology-learner/data/sample_docs.json')\n",
    "\n",
    "if not openai_embeddings_file.exists():\n",
    "    max_tokens = 7000\n",
    "    n_samples = 250\n",
    "    # get documents that are less than max_tokens\n",
    "\n",
    "    docs = {pmcid: fulltext[pmcid] for pmcid in fulltext if tokenlengths[pmcid] < max_tokens}\n",
    "    # get a random sample of 500 documents, indexex by pmcid\n",
    "\n",
    "    sample_pmcids = random.sample(list(docs.keys()), 250)\n",
    "    sample_docs = {pmcid: docs[pmcid] for pmcid in sample_pmcids}\n",
    "\n",
    "    # submit each document to openai for embedding\n",
    "\n",
    "    embeddings = {}\n",
    "    for pmcid, doc in tqdm(sample_docs.items()):\n",
    "        response = client.embeddings.create(\n",
    "            input=doc,\n",
    "            model=\"text-embedding-3-large\"\n",
    "        )\n",
    "        embeddings[pmcid] = response.data[0].embedding\n",
    "\n",
    "    # convert to numpy array, preserving pmcid order\n",
    "    pmcids = list(embeddings.keys())\n",
    "    embeddings_array = np.array([embeddings[pmcid] for pmcid in pmcids])\n",
    "\n",
    "    np.save(openai_embeddings_file, embeddings_array)\n",
    "    with open(sample_docs_file, 'w') as f:\n",
    "        json.dump(sample_docs, f)\n",
    "else:\n",
    "    print(f'loading openai embeddings from {openai_embeddings_file}')\n",
    "    embeddings_array = np.load(openai_embeddings_file)\n",
    "    with open(sample_docs_file, 'r') as f:\n",
    "        sample_docs = json.load(f)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute embeddings on full sample using fasttext unsupervised\n",
    "\n",
    "# first create text file with one document per line for the entire fulltext corpus\n",
    "fulltext_file = Path('/Users/poldrack/Dropbox/data/ontology-learner/data/fulltext.txt') \n",
    "if not fulltext_file.exists():\n",
    "    with open(fulltext_file, 'w') as f:\n",
    "        for doc in fulltext.values():\n",
    "            f.write(f'{doc}\\n')\n",
    "\n",
    "model_file = fulltext_file.with_suffix('.bin')\n",
    "if not model_file.exists():\n",
    "    # Skipgram model :\n",
    "    model = fasttext.train_unsupervised(fulltext_file.as_posix(), dim=300)\n",
    "    model.save_model(model_file.as_posix())\n",
    "else:\n",
    "    model = fasttext.load_model(model_file.as_posix())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:01<00:00, 147.30it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_fasttext = {}\n",
    "for pmcid, doc in tqdm(sample_docs.items()):\n",
    "    response = model.get_sentence_vector(doc.replace('\\n', ' '))\n",
    "    embeddings_fasttext[pmcid] = response\n",
    "\n",
    "# convert to numpy array, preserving pmcid order\n",
    "pmcids = list(embeddings_fasttext.keys())\n",
    "embeddings_fasttext_array = np.array([embeddings_fasttext[pmcid] for pmcid in pmcids])\n",
    "\n",
    "np.save('/Users/poldrack/Dropbox/data/ontology-learner/data/fasttext_embeddings.npy', embeddings_fasttext_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try using sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/Dropbox/code/ontology_learner/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 250/250 [00:06<00:00, 40.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')  # Using a good general-purpose model\n",
    "embeddings_st = {}\n",
    "for pmcid, doc in tqdm(sample_docs.items()):\n",
    "    # Get embeddings directly from the model\n",
    "    embedding = model.encode(doc)\n",
    "    embeddings_st[pmcid] = embedding\n",
    "\n",
    "# convert to numpy array, preserving pmcid order\n",
    "pmcids = list(embeddings_st.keys())\n",
    "embeddings_array_st = np.array([embeddings_st[pmcid] for pmcid in pmcids])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get representaitonal similarty for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_openai = np.corrcoef(embeddings_array)\n",
    "cc_fasttext = np.corrcoef(embeddings_fasttext_array)\n",
    "cc_st = np.corrcoef(embeddings_array_st)\n",
    "\n",
    "# %%\n",
    "assert cc_openai.shape == (250, 250)\n",
    "assert cc_fasttext.shape == (250, 250)\n",
    "assert cc_st.shape == (250, 250)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSA between openai and fasttext: 0.964\n",
      "RSA between openai and st: 0.825\n",
      "RSA between fasttext and st: 0.750\n"
     ]
    }
   ],
   "source": [
    "# get upper triangle of each matrix\n",
    "cc_openai_upper = np.triu(cc_openai)\n",
    "cc_fasttext_upper = np.triu(cc_fasttext)\n",
    "cc_st_upper = np.triu(cc_st)\n",
    "# %%\n",
    "\n",
    "rsa_openai_fasttext = np.corrcoef(cc_openai_upper.flatten(), cc_fasttext_upper.flatten())\n",
    "rsa_openai_st = np.corrcoef(cc_openai_upper.flatten(), cc_st_upper.flatten())\n",
    "rsa_fasttext_st = np.corrcoef(cc_fasttext_upper.flatten(), cc_st_upper.flatten())\n",
    "\n",
    "print(f'RSA between openai and fasttext: {rsa_openai_fasttext[0, 1]:.3f}')\n",
    "print(f'RSA between openai and st: {rsa_openai_st[0, 1]:.3f}')\n",
    "print(f'RSA between fasttext and st: {rsa_fasttext_st[0, 1]:.3f}')\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute fasttext embeddings for all of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132257/132257 [24:54<00:00, 88.47it/s] \n"
     ]
    }
   ],
   "source": [
    "embeddings_fasttext_fulltext = {}\n",
    "for pmcid, doc in tqdm(fulltext.items()):\n",
    "    response = model.get_sentence_vector(doc.replace('\\n', ' '))\n",
    "    embeddings_fasttext_fulltext[pmcid] = response\n",
    "\n",
    "# convert to numpy array, preserving pmcid order\n",
    "pmcids = list(embeddings_fasttext_fulltext.keys())\n",
    "embeddings_fasttext_fulltext_array = np.array([embeddings_fasttext_fulltext[pmcid] for pmcid in pmcids])\n",
    "\n",
    "np.save('/Users/poldrack/Dropbox/data/ontology-learner/data/fasttext_embeddings_fulltext.npy', embeddings_fasttext_fulltext_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/poldrack/Dropbox/data/ontology-learner/data/pmcids_fulltext.pkl', 'wb') as f:\n",
    "    pickle.dump(pmcids, f)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
