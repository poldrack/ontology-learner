{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task identified in learn_task_ontology.ipynb, do a pubmed abstract and retrieve up to 50 abstracts. Then fit a fasttext embedding model to the data using supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/Users/poldrack/Dropbox/data/ontology-learner/data\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pubmedutils.pubmed import (\n",
    "    get_pubmed_data, \n",
    "    parse_pubmed_record,\n",
    "    parse_pubmed_pubs\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import fasttext\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "datadir = Path(os.getenv('DATADIR'))\n",
    "print(datadir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load task entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir / 'gpt4/task_entries.json', 'r') as f:\n",
    "    task_entries = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8432 task results from /Users/poldrack/Dropbox/data/ontology-learner/data/gpt4/task_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if (datadir / 'gpt4/task_results.json').exists():\n",
    "    with open(datadir / 'gpt4/task_results.json', 'r') as f:\n",
    "        task_results = json.load(f)\n",
    "    print(f\"Loaded {len(task_results)} task results from {datadir / 'gpt4/task_results.json'}\")\n",
    "else:\n",
    "    print(f\"No task results found at {datadir / 'gpt4/task_results.json'}, retrieving new data\")\n",
    "    task_results = {}\n",
    "    errors = {}\n",
    "\n",
    "    for k, v in tqdm.tqdm(task_entries.items()):\n",
    "        if k in task_results:\n",
    "            continue\n",
    "        try:\n",
    "            d = get_pubmed_data(query=v['name'], email='poldrack@stanford.edu', retmax=50)\n",
    "            task_results[k] = parse_pubmed_pubs(d)\n",
    "        except Exception as e:\n",
    "            errors[k] = str(e)\n",
    "            task_results[k] = []\n",
    "        # sleep for 100 ms\n",
    "        time.sleep(0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_results = [k for k in task_results if len(task_results[k]) == 0]\n",
    "len(empty_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding model fitting\n",
    "\n",
    "We want to fit an embedding using fasttext based on the retrieved abstracts.  \n",
    "\n",
    "First let's try using supervised learning, where the labels are the task keys.  \n",
    "\n",
    "The first thing we need to do is to save the text out to a file with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dict = {}\n",
    "\n",
    "trainfile = open(datadir / 'gpt4/task_abstracts_train.txt', 'w')\n",
    "testfile = open(datadir / 'gpt4/task_abstracts_test.txt', 'w')\n",
    "\n",
    "for k, v in task_results.items():\n",
    "    if len(v) == 0:\n",
    "        continue\n",
    "    ctr = 0\n",
    "    label = '__label__' + k\n",
    "    for doi, pub in v.items():\n",
    "        if ctr < 40:\n",
    "            trainfile.write(f\"{label} {pub['title']} {pub['Abstract']}\\n\")\n",
    "        else:\n",
    "            testfile.write(f\"{label} {pub['title']} {pub['Abstract']}\\n\")\n",
    "        ctr += 1\n",
    "    \n",
    "trainfile.close()\n",
    "testfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/poldrack/Dropbox/data/ontology-learner/data/gpt4/task_abstracts_model_100dims.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ndims = 100\n",
    "infile = (datadir / 'gpt4/task_abstracts_train.txt').as_posix()\n",
    "modelfile = datadir / f'gpt4/task_abstracts_model_{ndims}dims.bin'\n",
    "\n",
    "if not modelfile.exists():\n",
    "    model = fasttext.train_supervised(input=infile, dim=ndims)\n",
    "\n",
    "    model.quantize(input=infile, retrain=True)\n",
    "\n",
    "    model.save_model(modelfile.as_posix())\n",
    "\n",
    "else:\n",
    "    print(f\"Loading model from {modelfile}\")\n",
    "    model = fasttext.load_model(modelfile.as_posix())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get task embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
