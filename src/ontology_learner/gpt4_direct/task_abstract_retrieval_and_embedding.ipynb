{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task identified in learn_task_ontology.ipynb, do a pubmed abstract and retrieve up to 50 abstracts. Then fit a fasttext embedding model to the data using supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/Dropbox/data/ontology-learner/data\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pubmedutils.pubmed import (\n",
    "    get_pubmed_data, \n",
    "    parse_pubmed_record,\n",
    "    parse_pubmed_pubs\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import fasttext\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "datadir = Path(os.getenv('DATADIR'))\n",
    "print(datadir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load task entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir / 'gpt4/task_entries.json', 'r') as f:\n",
    "    task_entries = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8432 task results from /Users/poldrack/Dropbox/data/ontology-learner/data/gpt4/task_results.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if (datadir / 'gpt4/task_results.json').exists():\n",
    "    with open(datadir / 'gpt4/task_results.json', 'r') as f:\n",
    "        task_results = json.load(f)\n",
    "    print(f\"Loaded {len(task_results)} task results from {datadir / 'gpt4/task_results.json'}\")\n",
    "else:\n",
    "    print(f\"No task results found at {datadir / 'gpt4/task_results.json'}, retrieving new data\")\n",
    "    task_results = {}\n",
    "    errors = {}\n",
    "\n",
    "    for k, v in tqdm.tqdm(task_entries.items()):\n",
    "        if k in task_results:\n",
    "            continue\n",
    "        try:\n",
    "            d = get_pubmed_data(query=v['name'], email='poldrack@stanford.edu', retmax=50)\n",
    "            task_results[k] = parse_pubmed_pubs(d)\n",
    "        except Exception as e:\n",
    "            errors[k] = str(e)\n",
    "            task_results[k] = []\n",
    "        # sleep for 100 ms\n",
    "        time.sleep(0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_results = [k for k in task_results if len(task_results[k]) == 0]\n",
    "len(empty_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding model fitting\n",
    "\n",
    "We want to fit an embedding using fasttext based on the retrieved abstracts.  \n",
    "\n",
    "First let's try using supervised learning, where the labels are the task keys.  \n",
    "\n",
    "The first thing we need to do is to save the text out to a file with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_dict = {}\n",
    "\n",
    "trainfile = open(datadir / 'gpt4/task_abstracts_train.txt', 'w')\n",
    "testfile = open(datadir / 'gpt4/task_abstracts_test.txt', 'w')\n",
    "\n",
    "for k, v in task_results.items():\n",
    "    if len(v) == 0:\n",
    "        continue\n",
    "    ctr = 0\n",
    "    label = '__label__' + k\n",
    "    for doi, pub in v.items():\n",
    "        if ctr < 40:\n",
    "            trainfile.write(f\"{label} {pub['title']} {pub['Abstract']}\\n\")\n",
    "        else:\n",
    "            testfile.write(f\"{label} {pub['title']} {pub['Abstract']}\\n\")\n",
    "        ctr += 1\n",
    "    \n",
    "trainfile.close()\n",
    "testfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 72M words\n",
      "Number of words:  296606\n",
      "Number of labels: 7775\n",
      "Progress: 100.0% words/sec/thread:   97836 lr:  0.000000 avg.loss:  0.851920 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For now we only support quantization of supervised models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         model \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mtrain_supervised(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39minfile, dim\u001b[38;5;241m=\u001b[39mndims)\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_model(modelfile\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dropbox/code/ontology_learner/.venv/lib/python3.12/site-packages/fasttext/FastText.py:363\u001b[0m, in \u001b[0;36m_FastText.quantize\u001b[0;34m(self, input, qout, cutoff, retrain, epoch, lr, thread, verbose, dsub, qnorm)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnorm\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: For now we only support quantization of supervised models"
     ]
    }
   ],
   "source": [
    "\n",
    "ndims = 100\n",
    "modeltype = 'unsupervised'\n",
    "infile = (datadir / 'gpt4/task_abstracts_train.txt').as_posix()\n",
    "modelfile = datadir / f'gpt4/task_abstracts_model_{ndims}dims_{modeltype}.bin'\n",
    "\n",
    "if not modelfile.exists():\n",
    "    if modeltype == 'unsupervised':\n",
    "        model = fasttext.train_unsupervised(input=infile, dim=ndims)\n",
    "    else:\n",
    "        model = fasttext.train_supervised(input=infile, dim=ndims)\n",
    "        model.quantize(input=infile, retrain=True)\n",
    "\n",
    "    model.save_model(modelfile.as_posix())\n",
    "\n",
    "else:\n",
    "    print(f\"Loading model from {modelfile}\")\n",
    "    model = fasttext.load_model(modelfile.as_posix())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get task embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
