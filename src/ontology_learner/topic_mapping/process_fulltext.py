# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# Process fmri pubmed abstracts by year

# %%

# code generated by ChatGPT

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import os
from nltk.stem import WordNetLemmatizer
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm
from ontology_learner.publication import Publication
import numpy as np
import json


# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet', quiet=True)


load_dotenv()

datadir = Path(os.getenv('DATADIR'))
print(datadir)

jsondir = datadir / 'json'


# %%

def get_stop_words():
    # Define the stop words
    stop_words = list(set(stopwords.words('english')))
    with open('additional_stopwords_topicmodel.txt', 'r') as f:
        additional_stopwords = [i.strip() for i in f.readlines()]
    return stop_words + additional_stopwords

def clean_text(sentences, stop_words):
    # Instantiate the wordnet lemmatizer
    lem = WordNetLemmatizer()

    cleaned_sentences = []
    for sentence in sentences:
        # Tokenize the sentence
        words = [i.lower() for i in word_tokenize(sentence)]

        # Remove stop words and stem the words
        cleaned_sentence = [lem.lemmatize(w) for w in words if w.isalpha()]
        cleaned_sentence = [
            w for w in cleaned_sentence if w not in stop_words and len(w) > 2
        ]
        cleaned_sentences.append(' '.join(cleaned_sentence))

    return cleaned_sentences

def list_contains_all_set_elements(lst, s):
    return set(s).issubset(lst)



# %%

# Read the papers

json_files = list(jsondir.glob('*.json'))
json_files = sorted(json_files)

sections_of_interest = set([
    'DISCUSS',
    'RESULTS',
    'ABSTRACT',
    'INTRO',
    'CONCL',
    'METHODS',
    'TITLE',
])

fulltext = {}
ctr = 0
stop_words = get_stop_words()

ctr = 0
cutoff = np.inf
for json_file in tqdm(json_files):
    p = Publication(json_file.stem, datadir=jsondir)
    p.parse_sections()
    if not list_contains_all_set_elements(p.sections.keys(), sections_of_interest):
        continue
    p_json = {}
    for section in sections_of_interest:
        if section in p.sections:
            p_json[section] = p.sections[section]
    fulltext[json_file.stem] = p_json
    if ctr > cutoff:
        break
    ctr += 1

with open(datadir / 'fulltext_sections.json', 'w') as f:
    json.dump(fulltext, f)


