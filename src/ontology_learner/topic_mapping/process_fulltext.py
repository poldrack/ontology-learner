# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# Process fmri pubmed abstracts by year

# %%

# code generated by ChatGPT

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pickle
from collections import namedtuple
import os
from nltk.stem import WordNetLemmatizer
import gensim
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm
from ontology_learner.publication import Publication
import numpy as np

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet', quiet=True)


load_dotenv()

datadir = Path(os.getenv('DATADIR'))
print(datadir)

jsondir = datadir / 'json'


# %%

def get_stop_words():
    # Define the stop words
    stop_words = list(set(stopwords.words('english')))
    with open('additional_stopwords_topicmodel.txt', 'r') as f:
        additional_stopwords = [i.strip() for i in f.readlines()]
    return stop_words + additional_stopwords

def clean_text(sentences, stop_words):
    # Instantiate the wordnet lemmatizer
    lem = WordNetLemmatizer()

    cleaned_sentences = []
    for sentence in sentences:
        # Tokenize the sentence
        words = [i.lower() for i in word_tokenize(sentence)]

        # Remove stop words and stem the words
        cleaned_sentence = [lem.lemmatize(w) for w in words if w.isalpha()]
        cleaned_sentence = [
            w for w in cleaned_sentence if w not in stop_words and len(w) > 2
        ]
        cleaned_sentences.append(' '.join(cleaned_sentence))

    return cleaned_sentences


# %%

# Read the papers

json_files = list(jsondir.glob('*.json'))
json_files = sorted(json_files)

sections_of_interest = [
    'DISCUSS',
    'RESULTS',
    'ABSTRACT',
    'INTRO',
    'CONCL',
    'METHODS',
    'TITLE',
]

fulltext = {}
ctr = 0
stop_words = get_stop_words()

ctr = 0
cutoff = np.inf
for json_file in tqdm(json_files):
    p = Publication(json_file.stem, datadir=jsondir)
    p.parse_sections()
    text = ''
    for section in sections_of_interest:
        if section in p.sections:
            text += ' ' + p.sections[section]
    fulltext[json_file.stem] = clean_text([text], stop_words)[0]
    if ctr > cutoff:
        break
    ctr += 1

with open(datadir / 'fulltext_cleaned.pkl', 'wb') as f:
    pickle.dump(fulltext, f)



# %%

bigram_file = datadir / 'bigram_model.pkl'
overwrite = True
min_count = 2

if bigram_file.exists() and not overwrite:
    bigram = gensim.models.Phrases.load(bigram_file)
else:
#  Get abstracts for each year
    bigram = gensim.models.Phrases(
        min_count=min_count
    )   # min_count determined by eyeball

    for pmcid, text in fulltext.items():
        bigram.add_vocab([text.split(' ')])

frozen_model = bigram.freeze()
frozen_model.save(bigram_file.as_posix())



# %%

bg_abstracts = {}
for pmcid, text in fulltext.items():
    split_text = text.split(' ')
    # replace underscores with dashes
    bg_abstracts[pmcid] = [j.replace('_', '-') 
                           for j in frozen_model[split_text]]

with open(datadir / 'bigrammed_cleaned_fulltext.pkl', 'wb') as f:
    pickle.dump(bg_abstracts, f)

